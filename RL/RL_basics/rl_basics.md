[Bellman Equation](#bellman-equation) 

[Policy Network Optimization (Actor) analysis](#policy-network-optimization-actor-analysis)

#### Reference
[Bellman Equation References](#be-references) 

[Policy Network Optimization (Actor) analysis references](#pno-references)

# Bellman Equation

The value of a state is the expected return when starting in that state and following the policy [Ghasemi, 2024].

## Existence of Iterative Solution

We begin with the Bellman expectation equation for the value function $V^\pi(s)$ under policy $\pi$:

$$
V^\pi(s) = r_\pi(s) + \gamma \sum_{s'} P_\pi(s' \mid s) V^\pi(s')
$$

where:

- $r_\pi(s)$ is the expected reward when in state $s$ and following policy $\pi$,
- $P_\pi(s' \mid s)$ is the probability of transitioning from state $s$ to state $s'$ under policy $\pi$,
- $\gamma$ is the discount factor.

This equation defines the value function $V^\pi(s)$ as the expected return starting from state $s$. The goal is to solve this equation for $V^\pi(s)$.

## Step 1: Define the Iterative Solution

An iterative solution involves updating an initial guess for the value function $V_0$ using the Bellman expectation operator $T^\pi$:

$$
V_{k+1} = T^\pi V_k
$$

where we define the Bellman operator $T^\pi$ as:

$$
(T^\pi V)(s) = r_\pi(s) + \gamma \sum_{s'} P_\pi(s' \mid s) V(s')
$$

The process is repeated until convergence, and the fixed point of this iteration will give the value function $V^\pi$.

## Step 2: Show that the Bellman Operator is a Contraction

We now show that the Bellman operator $T^\pi$ is a contraction with respect to the supremum norm. Let $V$ and $W$ be two value functions. We want to show that:

$$
\|T^\pi V - T^\pi W\|_\infty \leq \gamma \|V - W\|_\infty
$$

where the supremum norm $\|V - W\|_\infty$ is defined as:

$$
\|V - W\|_\infty = \max_{s \in \mathcal{S}} |V(s) - W(s)|
$$

Consider the difference between $T^\pi V(s)$ and $T^\pi W(s)$:

$$
|T^\pi V(s) - T^\pi W(s)| = \left| \gamma \sum_{s'} P_\pi(s' \mid s) (V(s') - W(s')) \right|
$$

Using the triangle inequality, we have:

$$
\leq \gamma \sum_{s'} P_\pi(s' \mid s) |V(s') - W(s')|
$$

Since $ |V(s') - W(s')| \leq \|V - W\|_\infty $ for all $s' \in \mathcal{S}$, we can further bound the sum:

$$
\leq \gamma \sum_{s'} P_\pi(s' \mid s) \|V - W\|_\infty
$$

Since the sum of the transition probabilities $ P_\pi(s' \mid s) $ over all $ s' $ is equal to 1, we have:

$$
\leq \gamma \|V - W\|_\infty
$$

Thus, we obtain:

$$
\|T^\pi V - T^\pi W\|_\infty \leq \gamma \|V - W\|_\infty
$$

## Step 3: Apply the Banach Fixed Point Theorem

Since $ \gamma < 1 $, this shows that $T^\pi$ is a contraction mapping with respect to the supremum norm.

By the **Banach Fixed Point Theorem**, since $T^\pi$ is a contraction, the sequence of value functions $V_k$ generated by the iteration $V_{k+1} = T^\pi V_k$ will converge to a unique fixed point $V^\pi$ as $k \to \infty$. This fixed point is the solution to the Bellman expectation equation:

$$
V^\pi(s) = r_\pi(s) + \gamma \sum_{s'} P_\pi(s' \mid s) V^\pi(s')
$$

## Conclusion

Thus, the iterative approach to solving the Bellman expectation equation converges to the optimal value function $V^\pi(s)$ under the given policy $\pi$. The convergence is guaranteed because the Bellman expectation operator $T^\pi$ is a contraction mapping in the supremum norm.

This proves that the iterative method for solving the Bellman equation will always converge to the unique fixed point, which corresponds to the value function under policy $\pi$.


## BE References

1. Sutton, R. S., & Barto, A. G. (2018). *Reinforcement Learning: An Introduction* (2nd ed.). MIT Press. 
2. EE 290 Theory of Multi-armed Bandits and Reinforcement Learning Lecture 17- 3/16/2021 Lecturer: Jiantao Jiao Lecture 17: Bellman Operators, Policy Iteration, and Value Iteration
3. Shiyu Zhao. Mathematical Foundations of Reinforcement Learning. Springer Singapore


# Policy Network Optimization (Actor) analysis

Before introducing the clipped objective used in PPO, it is important to address the inefficiency associated with sampling from the current policy. Since the policy changes after every update, collecting new data for each iteration is expensive and sample-inefficient. To overcome this, PPO reuses samples from the previous policy by applying *importance sampling*.

The policy gradient can then be estimated using:

$$
\nabla_\theta J(\theta) \approx \mathbb{E}_{(s,a) \sim \pi_{\theta_{\text{old}}}} \left[ \frac{\pi_\theta(a \mid s)}{\pi_{\theta_{\text{old}}}(a \mid s)} \, \nabla_\theta \log \pi_\theta(a \mid s) \, \hat{A}_t \right]
$$

Note that the gradient of the log-probability can be expressed as:

$$
\nabla_\theta \log \pi_\theta(a \mid s) = \frac{\nabla_\theta \pi_\theta(a \mid s)}{\pi_\theta(a \mid s)}
$$

This identity is fundamental to the derivation of the policy gradient using the likelihood ratio trick.

The **surrogate objective** used for policy optimization in PPO is then defined as:

$$
L^{\text{PG}}(\theta) = \mathbb{E}_{(s,a) \sim \pi_{\theta_{\text{old}}}} \left[ r_\theta(s,a) \, \hat{A}_t \right]
$$

where $ \hat{A}_t $ is the estimated advantage function, and the probability ratio is given by:

$$
r_\theta(s,a) = \frac{\pi_\theta(a \mid s)}{\pi_{\theta_{\text{old}}}(a \mid s)}
$$

To prevent overly large policy updates that may degrade performance, PPO introduces a clipped version of the objective:

$$
L^{\text{CLIP}}(\theta) = \mathbb{E}_{(s,a)} \left[ \min \left( r_\theta(s,a) \, \hat{A}_t, \, \text{clip}(r_\theta(s,a), 1 - \epsilon, 1 + \epsilon) \, \hat{A}_t \right) \right]
$$

This clipped surrogate objective constrains the size of the policy update, encouraging more stable training while still allowing for effective policy improvement.

## PNO References
- Schulman, J., Wolski, F., Dhariwal, P., Radford, A., & Klimov, O. (2017). *Proximal Policy Optimization Algorithms*. arXiv preprint [arXiv:1707.06347](https://arxiv.org/abs/1707.06347).
- Rubinstein, R. Y. (1981). *Simulation and the Monte Carlo Method*. Wiley. See Chapter 5.7 for details on importance sampling.
