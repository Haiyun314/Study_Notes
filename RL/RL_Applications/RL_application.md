## RL + LLM
### RLHF 
- reinforcement learning with feedback : align AI behavior with human values and preferences. 
- *** Reward *** : Annotators rank the responses generated by the large model for a given prompt.
- <img src='./images/Screenshot 2025-05-11 at 10.30.04.png' width ='500'/>

- ***PPO*** : Proximal Policy Optimization
- <img src='./images/Screenshot 2025-05-11 at 10.08.01.png' width= '500'/>

- ***DPO*** : Direct Preference Optimization
- <img src='./images/Screenshot 2025-05-11 at 10.31.17.png' width = '500'/>

- ***SimPO*** : Simpler Preference Optimization
- <img src='./images/Screenshot 2025-05-11 at 10.11.17.png' width = '500'/>

- ***KTO*** : Kahneman-Tversky Optimization
- <img src='./images/Screenshot 2025-05-11 at 10.10.57.png' width= '500'/>

## Continue ...